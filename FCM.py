import streamlit as st
import pandas as pd
import requests
import string
import re
from collections import Counter
from io import BytesIO

# --- å˜—è©¦åŒ¯å…¥å¥—ä»¶ ---
try:
    import xlsxwriter
    import jieba
    import jieba.analyse
except ImportError:
    pass

# --- 1. è¨­å®š API Key ---
USER_API_KEY = "AIzaSyBlj24gBVr3RJhkukS9p6yo5s2-WVBH2H0" 

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="AI é›™å¼•æ“æ–‡ç»åˆ†æ", layout="wide", page_icon="ğŸ›¡ï¸")
st.title("ğŸ›¡ï¸ AI é›™å¼•æ“æ–‡ç»åˆ†æå™¨ (æ°¸ä¸ç•¶æ©Ÿç‰ˆ)")
st.markdown("### é‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨ Google AIï¼Œè‹¥é¡åº¦é¡æ»¿(429)å‰‡è‡ªå‹•åˆ‡æ›è‡³æœ¬æ©Ÿæ¼”ç®—æ³•ã€‚")

# --- 3. æ ¸å¿ƒï¼šGoogle AI åˆ†æ ---
def analyze_with_google(text, key):
    # ä½¿ç”¨ gemini-1.5-flash (å…è²»é¡åº¦è¼ƒé«˜)
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={key}"
    headers = {'Content-Type': 'application/json'}
    prompt = f"ä»»å‹™ï¼šæ­¸ç´ 10 å€‹å­¸è¡“ç ”ç©¶æ§‹é¢é—œéµå­—ã€‚è¦å‰‡ï¼šåªåˆ—å‡ºåè©ï¼Œç”¨é “è™Ÿéš”é–‹ã€‚æ’é™¤ç„¡é—œè©å½™(å¦‚æ—¥æœŸã€ä¸‹åˆ)ã€‚å…§å®¹ï¼š{text[:5000]}"
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            return "SUCCESS", response.json()['candidates'][0]['content']['parts'][0]['text']
        elif response.status_code == 429:
            return "QUOTA_ERROR", "é¡åº¦é¡æ»¿"
        else:
            return "OTHER_ERROR", response.text
    except Exception as e:
        return "NET_ERROR", str(e)

# --- 4. æ ¸å¿ƒï¼šæœ¬æ©Ÿå‚™ç”¨æ¼”ç®—æ³• (Jieba) ---
def analyze_with_local(text):
    # é€™æ˜¯å‚™æ¡ˆï¼Œç•¶ AI æ›æ‰æ™‚ä½¿ç”¨
    # 1. å˜—è©¦ç”¨ jieba æŠ“é—œéµå­—
    try:
        keywords = jieba.analyse.extract_tags(text, topK=15, allowPOS=('n', 'vn', 'v'))
        return keywords
    except:
        # è¬ä¸€é€£ jieba éƒ½æ²’è£ï¼Œç”¨æœ€ç¬¨çš„æ–¹æ³•åˆ‡
        clean_text = re.sub(r'[^\u4e00-\u9fa5]', '', text)
        words = [clean_text[i:i+2] for i in range(len(clean_text)-1)]
        return [w for w, c in Counter(words).most_common(15)]

# --- 5. åˆ‡å‰²èˆ‡åŸ·è¡Œ ---
st.info("ğŸ‘‡ è«‹è²¼ä¸Šæ–‡ç»è³‡æ–™ (æ¯ç¯‡æ›è¡Œ)")
raw_text = st.text_area("æ–‡ç»è¼¸å…¥", height=200)

def parse_text(text):
    lines = text.strip().split('\n')
    return [{"title": line[:15], "content": line} for line in lines if len(line) > 5]

if st.button("ğŸš€ é–‹å§‹æ™ºæ…§åˆ†æ", type="primary"):
    if not raw_text:
        st.warning("è«‹å…ˆè²¼ä¸Šè³‡æ–™")
    else:
        status_msg = st.empty()
        status_msg.info("ğŸ¤– æ­£åœ¨å˜—è©¦å‘¼å« Google AI...")
        
        # 1. å…ˆè©¦è©¦çœ‹ Google
        status, result_text = analyze_with_google(raw_text, USER_API_KEY)
        
        final_keywords = []
        source_used = ""
        
        if status == "SUCCESS":
            status_msg.success("âœ… Google AI é€£ç·šæˆåŠŸï¼")
            final_keywords = [k.strip() for k in result_text.replace("\n", "ã€").split("ã€") if k.strip()]
            source_used = "Google AI"
        
        else:
            # 2. å¦‚æœ Google å¤±æ•— (429 æˆ–å…¶ä»–)ï¼Œå•Ÿå‹•å‚™ç”¨æ–¹æ¡ˆ
            error_reason = "é¡åº¦å·²æ»¿ (429)" if status == "QUOTA_ERROR" else "é€£ç·šå•é¡Œ"
            status_msg.warning(f"âš ï¸ Google AI æš«æ™‚ç„¡æ³•ä½¿ç”¨ ({error_reason})ï¼Œå·²è‡ªå‹•åˆ‡æ›è‡³ã€Œæœ¬æ©Ÿæ¼”ç®—æ³•ã€ç¹¼çºŒåˆ†æ...")
            final_keywords = analyze_with_local(raw_text)
            source_used = "æœ¬æ©Ÿæ¼”ç®—æ³• (å‚™ç”¨æ¨¡å¼)"
            
        # --- ä¸‹é¢æ˜¯è£½è¡¨ (ä¸ç®¡ç”¨å“ªç¨®æ–¹æ³•ï¼Œé€™è£¡éƒ½æœƒåŸ·è¡Œ) ---
        st.divider()
        st.markdown(f"**æœ¬æ¬¡åˆ†æä¾†æºï¼š{source_used}**")
        
        # è®“ä½¿ç”¨è€…ç¯©é¸
        selected_keywords = st.multiselect("åˆ†ææº–å‰‡ (å¯åˆªæ¸›)", options=final_keywords, default=final_keywords)
        
        if selected_keywords:
            lit_data = parse_text(raw_text)
            matrix = {}
            labels = []
            titles = []
            
            for i, item in enumerate(lit_data):
                lbl = string.ascii_uppercase[i % 26]
                labels.append(lbl)
                titles.append(item['title'])
                col_res = []
                for kw in selected_keywords:
                    if kw in item['content']: col_res.append("â—‹")
                    else: col_res.append("")
                matrix[lbl] = col_res
            
            df = pd.DataFrame(matrix, index=selected_keywords)
            df_legend = pd.DataFrame({"ä»£è™Ÿ": labels, "æ–‡ç»": titles})
            
            c1, c2 = st.columns([2, 1])
            with c1: st.dataframe(df, use_container_width=True)
            with c2: st.dataframe(df_legend, hide_index=True)
            
            # ä¸‹è¼‰
            output = BytesIO()
            try:
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    df.to_excel(writer, sheet_name='çŸ©é™£')
                    df_legend.to_excel(writer, sheet_name='å°ç…§è¡¨')
                st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", output.getvalue(), "analysis.xlsx")
            except:
                st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV", df.to_csv().encode('utf-8-sig'), "analysis.csv")
