import streamlit as st
import pandas as pd
import requests
import string
import re
from collections import Counter
from io import BytesIO

# --- å˜—è©¦åŒ¯å…¥å‚™ç”¨å¥—ä»¶ (é˜²å‘†) ---
try:
    import xlsxwriter
    import jieba
    import jieba.analyse
except ImportError:
    pass

# --- 1. è¨­å®š API Key ---
USER_API_KEY = "AIzaSyBlj24gBVr3RJhkukS9p6yo5s2-WVBH2H0" 

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="AI æ¨¡å‹æƒæèˆ‡åˆ†æ", layout="wide", page_icon="ğŸ“¡")

# åˆå§‹åŒ– Session State (ç”¨ä¾†è¨˜ä½æƒæåˆ°çš„æ¨¡å‹ï¼Œæ‰ä¸æœƒä¸€ç›´é‡è·‘)
if 'available_models' not in st.session_state:
    st.session_state.available_models = []
if 'scan_done' not in st.session_state:
    st.session_state.scan_done = False

# ==========================================
# ğŸ›‘ å·¦å´é‚Šæ¬„ï¼šæ¨¡å‹æƒæç«™
# ==========================================
with st.sidebar:
    st.header("ğŸ“¡ ç¬¬ä¸€æ­¥ï¼šæ¨¡å‹æƒæ")
    st.info("è«‹å…ˆé»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œæœå°‹ç›®å‰å¯ç”¨çš„ Google AI æ¨¡å‹ã€‚")
    
    # æƒæå‡½æ•¸
    def scan_google_models(key):
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={key}"
        try:
            response = requests.get(url)
            if response.status_code == 200:
                data = response.json()
                # ç¯©é¸å‡ºæ”¯æ´ generateContent çš„ gemini æ¨¡å‹
                valid_list = []
                for m in data.get('models', []):
                    if 'generateContent' in m.get('supportedGenerationMethods', []) and 'gemini' in m['name']:
                        # åªå–åå­—ï¼Œå»æ‰ 'models/' å‰ç¶´è®“ç•«é¢å¥½çœ‹é»
                        friendly_name = m['name'].replace("models/", "")
                        valid_list.append(friendly_name)
                return valid_list
            else:
                return []
        except:
            return []

    # æƒææŒ‰éˆ•
    if st.button("ğŸ” ç«‹å³æƒæå¯ç”¨æ¨¡å‹", type="primary"):
        with st.spinner("æ­£åœ¨é€£ç·š Google ä¼ºæœå™¨æŸ¥è©¢åå–®..."):
            found_models = scan_google_models(USER_API_KEY)
            
            if found_models:
                st.session_state.available_models = found_models
                st.session_state.scan_done = True
                st.success(f"æƒæå®Œæˆï¼æ‰¾åˆ° {len(found_models)} å€‹æ¨¡å‹ã€‚")
            else:
                st.error("âŒ æƒæå¤±æ•—ï¼šç„¡æ³•é€£ç·šæˆ–é‡‘é‘°ç„¡æ•ˆã€‚")
                st.session_state.available_models = []
    
    st.divider()
    
    # é¡¯ç¤ºé¸æ“‡é¸å–® (åªæœ‰æƒææˆåŠŸæ‰æœƒå‡ºç¾)
    selected_model = None
    if st.session_state.scan_done and st.session_state.available_models:
        st.subheader("âœ… è«‹é¸æ“‡ä¸€å€‹æ¨¡å‹ï¼š")
        selected_model = st.radio(
            "å»ºè­°é¸æ“‡ Flash (å¿«) æˆ– Pro (ç©©)ï¼š",
            st.session_state.available_models,
            index=0 # é è¨­é¸ç¬¬ä¸€å€‹
        )
        st.caption(f"ç›®å‰å·²é–å®šï¼š`{selected_model}`")
    elif st.session_state.scan_done and not st.session_state.available_models:
        st.warning("âš ï¸ ç„¡æ³•ä½¿ç”¨ Google æ¨¡å‹ï¼Œå°‡è‡ªå‹•åˆ‡æ›è‡³ã€Œæœ¬æ©Ÿæ¼”ç®—æ³•ã€ã€‚")
        selected_model = "Local (æœ¬æ©Ÿå‚™ç”¨)"
    else:
        st.markdown("ç­‰å¾…æƒæä¸­...")

# ==========================================
# ğŸ‘‰ å³å´ä¸»ç•«é¢ï¼šåªæœ‰é¸å¥½æ¨¡å‹æ‰æœƒé¡¯ç¤º
# ==========================================
st.title("ğŸ“„ æ–‡ç»åˆ†æå·¥ä½œå€")

if not st.session_state.scan_done:
    # å°šæœªæƒææ™‚çš„ç•«é¢
    st.info("â¬…ï¸ è«‹å…ˆåœ¨å·¦å´é»æ“Š **ã€ŒğŸ” ç«‹å³æƒæå¯ç”¨æ¨¡å‹ã€** é–‹å§‹ã€‚")
    st.markdown("é€™æ¨£å¯ä»¥ç¢ºä¿æˆ‘å€‘æ‰¾åˆ°ä¸€å€‹ã€Œæœ‰ç©ºã€çš„æ¨¡å‹ï¼Œé¿å…è¼¸å…¥è³‡æ–™å¾Œæ‰ç™¼ç¾é€£ç·šå¤±æ•—ã€‚")

else:
    # æƒæå®Œæˆï¼Œé¡¯ç¤ºè¼¸å…¥æ¡†
    st.success(f"ğŸš€ ç³»çµ±æº–å‚™å°±ç·’ï¼ç›®å‰ä½¿ç”¨æ ¸å¿ƒï¼š**{selected_model if selected_model else 'æœ¬æ©Ÿæ¼”ç®—æ³•'}**")
    
    raw_text = st.text_area("è«‹åœ¨æ­¤è²¼ä¸Šæ–‡ç»è³‡æ–™ (æ¯ç¯‡è«‹æ›è¡Œ)ï¼š", height=300, placeholder="å°‡æ‘˜è¦è²¼åœ¨é€™è£¡...")

    # --- åˆ†æå‡½æ•¸ ---
    def run_analysis(text, model_name):
        # å¦‚æœæ˜¯æœ¬æ©Ÿæ¨¡å¼
        if model_name == "Local (æœ¬æ©Ÿå‚™ç”¨)":
            try:
                return jieba.analyse.extract_tags(text, topK=15, allowPOS=('n', 'vn', 'v'))
            except:
                clean = re.sub(r'[^\u4e00-\u9fa5]', '', text)
                words = [clean[i:i+2] for i in range(len(clean)-1)]
                return [w for w, c in Counter(words).most_common(15)]
        
        # å¦‚æœæ˜¯ Google æ¨¡å¼
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={USER_API_KEY}"
        headers = {'Content-Type': 'application/json'}
        prompt = f"ä»»å‹™ï¼šæ­¸ç´ 10 å€‹å­¸è¡“ç ”ç©¶æ§‹é¢é—œéµå­—ã€‚è¦å‰‡ï¼šåªåˆ—å‡ºåè©ï¼Œç”¨é “è™Ÿéš”é–‹ã€‚æ’é™¤ç„¡é—œè©å½™(å¦‚æ—¥æœŸã€ä¸‹åˆ)ã€‚å…§å®¹ï¼š{text[:5000]}"
        data = {"contents": [{"parts": [{"text": prompt}]}]}
        
        try:
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text']
            elif response.status_code == 429:
                return "QUOTA_FULL"
            else:
                return None
        except:
            return None

    def parse_text(text):
        lines = text.strip().split('\n')
        return [{"title": line[:15], "content": line} for line in lines if len(line) > 5]

    # åŸ·è¡ŒæŒ‰éˆ•
    if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
        if not raw_text:
            st.warning("è«‹å…ˆè¼¸å…¥è³‡æ–™ï¼")
        else:
            keywords = []
            
            with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {selected_model} é€²è¡Œåˆ†æ..."):
                result = run_analysis(raw_text, selected_model)
                
                if result == "QUOTA_FULL":
                    st.error("âŒ å“å‘€ï¼é€™å€‹æ¨¡å‹çš„é¡åº¦å‰›å¥½æ»¿äº† (429)ã€‚")
                    st.info("ğŸ’¡ å»ºè­°ï¼šè«‹åœ¨å·¦å´æ›å¦ä¸€å€‹æ¨¡å‹è©¦è©¦çœ‹ï¼ˆä¾‹å¦‚å¾ Flash æ›æˆ Proï¼‰ã€‚")
                elif result and isinstance(result, str):
                    # Google æˆåŠŸå›å‚³å­—ä¸²
                    keywords = [k.strip() for k in result.replace("\n", "ã€").split("ã€") if k.strip()]
                    st.success("âœ… AI åˆ†ææˆåŠŸï¼")
                elif isinstance(result, list):
                    # æœ¬æ©Ÿå›å‚³åˆ—è¡¨
                    keywords = result
                    st.success("âœ… æœ¬æ©Ÿé‹ç®—æˆåŠŸï¼")
                else:
                    st.error("âŒ é€£ç·šç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼Œè«‹å˜—è©¦åˆ‡æ›å…¶ä»–æ¨¡å‹ã€‚")

            # --- é¡¯ç¤ºçµæœ ---
            if keywords:
                final_keywords = st.multiselect("åˆ†ææº–å‰‡ (å¯èª¿æ•´)", options=keywords, default=keywords)
                
                if final_keywords:
                    lit_data = parse_text(raw_text)
                    matrix = {}
                    labels = []
                    titles = []
                    
                    for i, item in enumerate(lit_data):
                        lbl = string.ascii_uppercase[i % 26]
                        labels.append(lbl)
                        titles.append(item['title'])
                        col_res = ["â—‹" if k in item['content'] else "" for k in final_keywords]
                        matrix[lbl] = col_res
                    
                    df = pd.DataFrame(matrix, index=final_keywords)
                    df_legend = pd.DataFrame({"ä»£è™Ÿ": labels, "æ–‡ç»": titles})
                    
                    st.divider()
                    c1, c2 = st.columns([2, 1])
                    with c1: st.dataframe(df, use_container_width=True)
                    with c2: st.dataframe(df_legend, hide_index=True)
                    
                    output = BytesIO()
                    try:
                        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                            df.to_excel(writer, sheet_name='çŸ©é™£')
                            df_legend.to_excel(writer, sheet_name='å°ç…§è¡¨')
                        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", output.getvalue(), "analysis.xlsx")
                    except:
                        st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV", df.to_csv().encode('utf-8-sig'), "analysis.csv")
