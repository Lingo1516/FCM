import streamlit as st
import pandas as pd
import requests
import string
import re
import time # é€™æ˜¯é—œéµï¼Œç”¨ä¾†æ§åˆ¶é€Ÿåº¦
from collections import Counter
from io import BytesIO

# --- å˜—è©¦åŒ¯å…¥å‚™ç”¨å¥—ä»¶ ---
try:
    import xlsxwriter
    import jieba
    import jieba.analyse
except ImportError:
    pass

# --- 1. è¨­å®š API Key ---
USER_API_KEY = "AIzaSyBlj24gBVr3RJhkukS9p6yo5s2-WVBH2H0" 

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="AI æ™ºæ…§ç¯©é¸åˆ†æ", layout="wide", page_icon="ğŸ•µï¸")

if 'verified_models' not in st.session_state:
    st.session_state.verified_models = []
if 'filter_done' not in st.session_state:
    st.session_state.filter_done = False

# ==========================================
# ğŸ›‘ å·¦å´é‚Šæ¬„ï¼šæ™ºæ…§ç¯©é¸ç«™
# ==========================================
with st.sidebar:
    st.header("ğŸ•µï¸ ç¬¬ä¸€æ­¥ï¼šæ™ºæ…§ç¯©é¸")
    st.info("é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œç³»çµ±æœƒã€Œæ…¢é€Ÿã€é€ä¸€æ¸¬è©¦ï¼Œåªå¹«æ‚¨ç•™ä¸‹çœŸæ­£èƒ½ç”¨çš„æ¨¡å‹ã€‚")
    
    # æ¸¬è©¦å–®ä¸€æ¨¡å‹å‡½æ•¸
    def test_single_model(key, model_name):
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={key}"
        headers = {'Content-Type': 'application/json'}
        data = {"contents": [{"parts": [{"text": "Hi"}]}]}
        try:
            # è¨­å®šè¶…æ™‚
            response = requests.post(url, headers=headers, json=data, timeout=5)
            if response.status_code == 200:
                return True
            else:
                return False
        except:
            return False

    # åŸ·è¡Œç¯©é¸æŒ‰éˆ•
    if st.button("ğŸ” é–‹å§‹è‡ªå‹•ç¯©é¸ (ç´„éœ€ 10 ç§’)", type="primary"):
        st.session_state.verified_models = []
        
        # æˆ‘å€‘åªæŒ‘é€™å¹¾å€‹ã€Œç²¾è‹±æ¨¡å‹ã€ä¾†æ¸¬ï¼Œä¸è¦æ¸¬åƒåœ¾æ¨¡å‹æµªè²»æ™‚é–“
        candidates = [
            "gemini-1.5-flash",       # æœ€å¿«ã€æœ€ç©©
            "gemini-1.5-pro",         # æœ€è°æ˜
            "gemini-2.0-flash",       # æœ€æ–°ç‰ˆ
            "gemini-2.0-flash-lite-preview-02-05", # é è¦½è¼•é‡ç‰ˆ(é€šå¸¸æ²’äººç”¨ï¼Œé¡åº¦å¤š)
            "gemini-1.0-pro",         # ç¶“å…¸èˆŠç‰ˆ
            "gemini-1.5-flash-8b"     # æ¥µé€Ÿç‰ˆ
        ]
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        found_count = 0
        
        for i, model in enumerate(candidates):
            status_text.markdown(f"æ­£åœ¨æ¸¬è©¦ï¼š`{model}` ...")
            
            # 1. æ¸¬è©¦
            is_alive = test_single_model(USER_API_KEY, model)
            
            # 2. åˆ¤å®š
            if is_alive:
                st.session_state.verified_models.append(model)
                found_count += 1
                st.toast(f"âœ… {model} å¯ç”¨ï¼")
            
            # 3. æ›´æ–°é€²åº¦
            progress_bar.progress((i + 1) / len(candidates))
            
            # 4. ã€é—œéµã€‘æš«åœ 1.5 ç§’ï¼Œé¿å…è¢« Google é– IP
            time.sleep(1.5)
            
        st.session_state.filter_done = True
        status_text.text("ç¯©é¸å®Œæˆï¼")
        
        if found_count == 0:
            st.error("âŒ å…¨éƒ¨å¿™ç·šä¸­ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–ç”¨æœ¬æ©Ÿæ¨¡å¼ã€‚")

    st.divider()
    
    # é¡¯ç¤ºã€Œä¹¾æ·¨ã€çš„é¸å–®
    final_selection = None
    
    if st.session_state.filter_done:
        if st.session_state.verified_models:
            st.success(f"ğŸ‰ æˆåŠŸæ‰¾åˆ° {len(st.session_state.verified_models)} å€‹å¯ç”¨æ¨¡å‹ï¼")
            st.caption("ä»¥ä¸‹åˆ—è¡¨ä¿è­‰å‰›å‰›æ¸¬è©¦æ˜¯ç¶ ç‡ˆçš„ï¼š")
            final_selection = st.radio(
                "è«‹é¸æ“‡ä¸€å€‹é–‹å§‹åˆ†æï¼š",
                st.session_state.verified_models
            )
        else:
            st.warning("âš ï¸ ç‚ºäº†ä¸è®“ä½ ç©ºæ‰‹è€Œæ­¸ï¼Œå·²è‡ªå‹•åˆ‡æ›è‡³ã€Œæœ¬æ©Ÿå‚™ç”¨æ¨¡å¼ã€ã€‚")
            final_selection = "Local (æœ¬æ©Ÿå‚™ç”¨)"
    else:
        st.markdown("ç­‰å¾…ç¯©é¸ä¸­...")

# ==========================================
# ğŸ‘‰ å³å´ä¸»ç•«é¢
# ==========================================
st.title("ğŸ“„ æ–‡ç»åˆ†æå·¥ä½œå€")

if not st.session_state.filter_done:
    st.info("â¬…ï¸ è«‹å…ˆåœ¨å·¦å´é»æ“Š **ã€ŒğŸ” é–‹å§‹è‡ªå‹•ç¯©é¸ã€**ã€‚")
    st.markdown("""
    **é€™å€‹ç‰ˆæœ¬æœƒè‡ªå‹•å¹«æ‚¨ï¼š**
    1. æ¸¬è©¦ç›®å‰æœ€ç†±é–€çš„ 6 å€‹æ¨¡å‹ã€‚
    2. è‡ªå‹•éæ¿¾æ‰å£æ‰çš„ã€é¡åº¦æ»¿çš„ã€‚
    3. **åªåˆ—å‡ºèƒ½ç”¨çš„çµ¦æ‚¨é¸**ã€‚
    """)
else:
    # åªæœ‰ç¯©é¸éæ‰æœƒé¡¯ç¤ºé€™è£¡
    st.success(f"ğŸš€ å·²é–å®šæ ¸å¿ƒï¼š**{final_selection}**")
    
    raw_text = st.text_area("è«‹åœ¨æ­¤è²¼ä¸Šæ–‡ç»è³‡æ–™ (æ¯ç¯‡è«‹æ›è¡Œ)ï¼š", height=300)

    # åˆ†æå‡½æ•¸
    def run_analysis_final(text, model_name):
        # æœ¬æ©Ÿæ¨¡å¼
        if "Local" in model_name:
            try:
                return jieba.analyse.extract_tags(text, topK=15, allowPOS=('n', 'vn', 'v'))
            except:
                clean = re.sub(r'[^\u4e00-\u9fa5]', '', text)
                words = [clean[i:i+2] for i in range(len(clean)-1)]
                return [w for w, c in Counter(words).most_common(15)]

        # Google æ¨¡å¼
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={USER_API_KEY}"
        headers = {'Content-Type': 'application/json'}
        prompt = f"ä»»å‹™ï¼šæ­¸ç´ 10 å€‹å­¸è¡“ç ”ç©¶æ§‹é¢é—œéµå­—ã€‚è¦å‰‡ï¼šåªåˆ—å‡ºåè©ï¼Œç”¨é “è™Ÿéš”é–‹ã€‚æ’é™¤ç„¡é—œè©å½™(å¦‚æ—¥æœŸã€ä¸‹åˆ)ã€‚å…§å®¹ï¼š{text[:5000]}"
        data = {"contents": [{"parts": [{"text": prompt}]}]}
        
        try:
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                return response.json()['candidates'][0]['content']['parts'][0]['text']
            else:
                return None
        except:
            return None

    def parse_text(text):
        lines = text.strip().split('\n')
        return [{"title": line[:15], "content": line} for line in lines if len(line) > 5]

    if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
        if not raw_text:
            st.warning("è«‹å…ˆè¼¸å…¥è³‡æ–™ï¼")
        else:
            keywords = []
            with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {final_selection} åˆ†æ..."):
                res = run_analysis_final(raw_text, final_selection)
                
                if isinstance(res, str): # Google å›å‚³å­—ä¸²
                    keywords = [k.strip() for k in res.replace("\n", "ã€").split("ã€") if k.strip()]
                    st.success("âœ… åˆ†ææˆåŠŸ")
                elif isinstance(res, list): # æœ¬æ©Ÿå›å‚³ List
                    keywords = res
                    st.success("âœ… æœ¬æ©Ÿé‹ç®—æˆåŠŸ")
                else:
                    st.error("âŒ åˆ†æå¤±æ•—ï¼Œè©²æ¨¡å‹å¯èƒ½å‰›å¥½é¡åº¦ç”¨ç›¡ï¼Œè«‹å·¦å´æ›ä¸€å€‹è©¦è©¦ã€‚")

            if keywords:
                final_keywords = st.multiselect("åˆ†ææº–å‰‡", options=keywords, default=keywords)
                if final_keywords:
                    lit_data = parse_text(raw_text)
                    matrix = {}
                    labels = []
                    titles = []
                    for i, item in enumerate(lit_data):
                        lbl = string.ascii_uppercase[i % 26]
                        labels.append(lbl)
                        titles.append(item['title'])
                        matrix[lbl] = ["â—‹" if k in item['content'] else "" for k in final_keywords]
                    
                    df = pd.DataFrame(matrix, index=final_keywords)
                    df_legend = pd.DataFrame({"ä»£è™Ÿ": labels, "æ–‡ç»": titles})
                    
                    c1, c2 = st.columns([2, 1])
                    with c1: st.dataframe(df, use_container_width=True)
                    with c2: st.dataframe(df_legend, hide_index=True)
                    
                    output = BytesIO()
                    try:
                        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                            df.to_excel(writer, sheet_name='çŸ©é™£')
                            df_legend.to_excel(writer, sheet_name='å°ç…§è¡¨')
                        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", output.getvalue(), "analysis.xlsx")
                    except:
                        st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV", df.to_csv().encode('utf-8-sig'), "analysis.csv")
