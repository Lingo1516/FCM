import streamlit as st
import pandas as pd
import requests
import json
import string
import re
from io import BytesIO

# --- 1. åŸºç¤è¨­å®š ---
st.set_page_config(page_title="MCDM å…¨åŠŸèƒ½åˆ†æ (å«å‡ºè™•è¨»è¨˜)", layout="wide", page_icon="ğŸ’")

# --- 2. å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("ğŸ’ å…¨åŠŸèƒ½è¨­å®š")
    st.info("æ­¤ç‰ˆæœ¬å·²åœ¨ã€ŒåŸå§‹è¡¨ã€èˆ‡ã€Œæ”¶æ–‚è¡¨ã€çš„æœ€å³å´å¢åŠ ã€ä½œè€…ä»£è™Ÿã€‘æ¬„ä½ã€‚")
    
    api_key = st.text_input("Google API Key", type="password")
    st.divider()
    thesis_topic = st.text_input("è«–æ–‡é¡Œç›®ï¼š", value="é¤é£²æ¥­å°å…¥ AI æœå‹™ä¹‹è©•ä¼°æº–å‰‡")
    
    c1, c2 = st.columns(2)
    with c1:
        pool_size = st.number_input("åŸå§‹ç™¼æ•£æ•¸é‡", value=50)
    with c2:
        target_size = st.number_input("æœ€çµ‚æ”¶æ–‚æ•¸é‡", value=15)

# --- 3. è‡ªå‹•é©é…æ¨¡å‹ ---
def get_best_model(key):
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={key}"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            models = response.json().get('models', [])
            # å„ªå…ˆæ¬Šï¼šPro (é‚è¼¯å¥½) > Flash (é€Ÿåº¦å¿«)
            for m in models:
                if 'gemini-1.5-pro' in m['name']: return m['name']
            for m in models:
                if 'gemini-1.5-flash' in m['name']: return m['name']
            for m in models:
                if 'gemini' in m['name'] and 'generateContent' in m.get('supportedGenerationMethods', []):
                    return m['name']
            return None
        return None
    except:
        return None

# --- 4. æ ¸å¿ƒåˆ†æé‚è¼¯ (æ›´æ–° Prompt ä»¥ç´¢å– Step 1 çš„å‡ºè™•) ---
def run_all_in_one_analysis(text, key, model_name, topic, pool_n, target_n):
    url = f"https://generativelanguage.googleapis.com/v1beta/{model_name}:generateContent?key={key}"
    headers = {'Content-Type': 'application/json'}
    
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹ MCDM ç ”ç©¶å°ˆå®¶ã€‚é¡Œç›®ï¼š{topic}ã€‚
    è«‹åŸ·è¡Œå®Œæ•´çš„ã€Œç™¼æ•£ -> æ”¶æ–‚ -> çŸ©é™£ã€æµç¨‹ã€‚

    ã€ä»»å‹™è¦æ±‚ã€‘ï¼š
    1. è¾¨è­˜æ–‡ç»ä¸¦ç·¨è™Ÿ (ID 0, 1, 2...)ï¼Œè½‰ç‚º APA æ ¼å¼ã€‚
    2. Step 1 (Pooling): å¾æ–‡ä¸­æ‰¾å‡ºç´„ {pool_n} å€‹ã€ŒåŸå§‹ç´°é …æº–å‰‡ã€ã€‚
       **é‡è¦ï¼š** é‡å°æ¯ä¸€å€‹åŸå§‹ç´°é …ï¼Œè«‹æ¨™è¨»æ˜¯å“ªå¹¾ç¯‡æ–‡ç»æåˆ°çš„ (Paper IDs)ã€‚
    3. Step 2 (Convergence): å°‡å…¶æ­¸ç´ç‚º {target_n} å€‹ã€Œæœ€çµ‚æº–å‰‡ã€ã€‚
       - èªªæ˜æ¯å€‹æœ€çµ‚æº–å‰‡æ˜¯ç”±å“ªäº›åŸå§‹é …ç›®åˆä½µçš„ã€‚
       - èªªæ˜åˆä½µ/æ”¶æ–‚çš„ç†ç”± (Reasoning)ã€‚
       - æ¨™è¨»æ¯å€‹æœ€çµ‚æº–å‰‡å‡ºç¾åœ¨å“ªå¹¾ç¯‡è«–æ–‡ä¸­ (Paper IDs)ã€‚

    ã€å›å‚³ JSON æ ¼å¼ (åš´æ ¼éµå®ˆ)ã€‘ï¼š
    {{
      "papers": [
        {{ "id": 0, "apa": "ä½œè€…A (2024). æ¨™é¡Œ..." }},
        {{ "id": 1, "apa": "ä½œè€…B (2023). æ¨™é¡Œ..." }}
      ],
      "step1_raw_pool": [
        {{ "name": "åŸå§‹ç´°é …1", "matched_ids": [0, 1] }},
        {{ "name": "åŸå§‹ç´°é …2", "matched_ids": [2] }},
        ... (ç´„ {pool_n} å€‹)
      ],
      "step2_convergence": [
        {{
          "id": 1,
          "final_name": "æœ€çµ‚æº–å‰‡åç¨±",
          "source_raw_items": ["ç´°é …1", "ç´°é …5"],
          "reasoning": "å› ç‚ºçš†æ¶‰åŠè²¡å‹™æ”¯å‡º...",
          "matched_paper_ids": [0, 2] 
        }},
        ... (å…± {target_n} å€‹)
      ]
    }}
    
    æ–‡ç»å…§å®¹ï¼š
    {text[:13000]}
    """
    
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            try:
                res_text = response.json()['candidates'][0]['content']['parts'][0]['text']
                match = re.search(r'\{.*\}', res_text, re.DOTALL)
                if match:
                    return "OK", json.loads(match.group(0))
                else:
                    return "ERROR", "JSON è§£æå¤±æ•—"
            except:
                return "ERROR", "AI å›å‚³çµæ§‹ç•°å¸¸"
        else:
            return "ERROR", f"API Error: {response.status_code}"
    except Exception as e:
        return "ERROR", str(e)

# --- 5. ä¸»ç•«é¢ ---
st.title("ğŸ’ MCDM å®Œæ•´ç ”ç©¶å ±å‘Š (å«å‡ºè™•è¨»è¨˜)")

raw_text = st.text_area("è«‹è²¼ä¸Šæ–‡ç»æ‘˜è¦ï¼š", height=250)

if st.button("ğŸš€ åŸ·è¡Œå…¨åŠŸèƒ½åˆ†æ", type="primary"):
    if not api_key:
        st.error("âŒ è«‹è¼¸å…¥ Key")
    elif not raw_text:
        st.warning("âš ï¸ è«‹è¼¸å…¥æ–‡ç»")
    else:
        with st.spinner("ğŸ” AI æ­£åœ¨è™•ç†ï¼šç™¼æ•£(å«å‡ºè™•) -> æ”¶æ–‚(å«å‡ºè™•) -> çŸ©é™£å»ºæ§‹..."):
            valid_model = get_best_model(api_key)
            
            if not valid_model:
                st.error("âŒ æ‰¾ä¸åˆ°å¯ç”¨æ¨¡å‹")
            else:
                status, result = run_all_in_one_analysis(raw_text, api_key, valid_model, thesis_topic, pool_size, target_size)
                
                if status == "OK":
                    st.success("âœ… åˆ†æå®Œæˆï¼")
                    
                    # æº–å‚™è³‡æ–™
                    papers = result.get("papers", [])
                    raw_pool = result.get("step1_raw_pool", [])
                    conv_data = result.get("step2_convergence", [])
                    
                    # å»ºç«‹ä»£è™Ÿå°ç…§ Map (id -> A, B, C...)
                    id_to_code = {}
                    legend_rows = []
                    for idx, p in enumerate(papers):
                        code = string.ascii_uppercase[idx % 26]
                        id_to_code[p['id']] = code
                        legend_rows.append({"ä»£è™Ÿ": code, "æ–‡ç»ä¾†æº (APA)": p['apa']})
                    
                    df_legend = pd.DataFrame(legend_rows)
                    
                    # --- å»ºç«‹ 4 å€‹åˆ†é  ---
                    t1, t2, t3, t4 = st.tabs([
                        "1ï¸âƒ£ Step 1: åŸå§‹åˆ—è¡¨ (50)", 
                        "2ï¸âƒ£ Step 2: æ”¶æ–‚é‚è¼¯ (15)", 
                        "3ï¸âƒ£ Step 3: åˆ†æçŸ©é™£åœ–", 
                        "4ï¸âƒ£ æ–‡ç»ä»£è™Ÿå°ç…§"
                    ])
                    
                    # Tab 1: åŸå§‹æ±  (å¢åŠ å‡ºè™•æ¬„ä½)
                    with t1:
                        if raw_pool:
                            raw_rows = []
                            for i, item in enumerate(raw_pool):
                                # åˆ¤æ–· item æ˜¯å­—ä¸²é‚„æ˜¯å­—å…¸ (ç‚ºäº†ç›¸å®¹æ€§)
                                name = item["name"] if isinstance(item, dict) else str(item)
                                ids = item.get("matched_ids", []) if isinstance(item, dict) else []
                                codes = [id_to_code.get(pid, "?") for pid in ids]
                                codes.sort()
                                
                                raw_rows.append({
                                    "åºè™Ÿ": i + 1,
                                    "åŸå§‹ç´°é …æº–å‰‡": name,
                                    "å‡ºè™•ä»£è™Ÿ": ", ".join(codes)  # é€™è£¡å°±æ˜¯ä½ è¦çš„ A, B, C
                                })
                            
                            df_raw = pd.DataFrame(raw_rows)
                            st.dataframe(df_raw, hide_index=True, use_container_width=True)
                        else:
                            st.warning("ç„¡è³‡æ–™")
                            
                    # Tab 2: æ”¶æ–‚é‚è¼¯ (å¢åŠ å‡ºè™•æ¬„ä½)
                    with t2:
                        conv_rows = []
                        for item in conv_data:
                            # æ‰¾å‡ºå°æ‡‰çš„ä»£è™Ÿ
                            ids = item.get("matched_paper_ids", [])
                            codes = [id_to_code.get(pid, "?") for pid in ids]
                            codes.sort()
                            
                            conv_rows.append({
                                "åºè™Ÿ": item.get("id"),
                                "æœ€çµ‚æº–å‰‡": item.get("final_name"),
                                "æ¶µè“‹ä¹‹åŸå§‹ç´°é …": ", ".join(item.get("source_raw_items", [])),
                                "æ”¶æ–‚/åˆä½µç†ç”±": item.get("reasoning"),
                                "å‡ºè™•ä»£è™Ÿ": ", ".join(codes) # é€™è£¡å°±æ˜¯ä½ è¦çš„ A, C, D
                            })
                        df_conv = pd.DataFrame(conv_rows)
                        st.dataframe(df_conv, hide_index=True, use_container_width=True)
                        
                    # Tab 3: çŸ©é™£åœ– (ä¿æŒä¸è®Šï¼Œå› ç‚ºé€™æ˜¯ä½ è¦çš„é»‘é»é»)
                    with t3:
                        matrix_rows = []
                        all_codes = [d["ä»£è™Ÿ"] for d in legend_rows]
                        
                        for item in conv_data:
                            row = {"æœ€çµ‚æº–å‰‡åç¨±": item.get("final_name")}
                            matched = item.get("matched_paper_ids", [])
                            
                            for code in all_codes:
                                target_id = -1
                                for pid, pcode in id_to_code.items():
                                    if pcode == code: target_id = pid
                                
                                if target_id in matched:
                                    row[code] = "â—"
                                else:
                                    row[code] = ""
                            matrix_rows.append(row)
                        
                        df_matrix = pd.DataFrame(matrix_rows)
                        st.dataframe(df_matrix, hide_index=True, use_container_width=True)
                        
                    # Tab 4: æ–‡ç»å°ç…§
                    with t4:
                        st.dataframe(df_legend, hide_index=True, use_container_width=True)
                        
                    # --- ä¸‹è¼‰ ---
                    st.divider()
                    output = BytesIO()
                    try:
                        import xlsxwriter
                        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                            if raw_pool: df_raw.to_excel(writer, sheet_name='Step1_åŸå§‹(50)', index=False)
                            if conv_data: df_conv.to_excel(writer, sheet_name='Step2_æ”¶æ–‚(15)', index=False)
                            if conv_data: df_matrix.to_excel(writer, sheet_name='Step3_çŸ©é™£', index=False)
                            df_legend.to_excel(writer, sheet_name='æ–‡ç»å°ç…§è¡¨', index=False)
                        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel (å«å‡ºè™•è¨»è¨˜)", output.getvalue(), "mcdm_full_report.xlsx", type="primary")
                    except:
                        st.error("Excel åŒ¯å‡ºæ¨¡çµ„éŒ¯èª¤")

                else:
                    st.error("åˆ†æå¤±æ•—")
                    st.code(result)
