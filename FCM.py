import streamlit as st
import pandas as pd
import string
import re
from collections import Counter
from io import BytesIO

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="æ™ºæ…§æº–å‰‡èƒå–å™¨", layout="wide", page_icon="ğŸ§ ")

st.title("ğŸ§  æ™ºæ…§æº–å‰‡èƒå–èˆ‡çŸ©é™£ç”Ÿæˆ")
st.markdown("### è‡ªå‹•å¾æ‚¨çš„è³‡æ–™ä¸­ã€Œæç…‰ã€å‡ºé—œéµæº–å‰‡ï¼Œä¸¦éæ¿¾æ‰ç„¡æ„ç¾©çš„é›œå­—ã€‚")

# --- 1. è¼¸å…¥å€ ---
st.info("ğŸ‘‡ è«‹è²¼ä¸Šæ‚¨çš„æ–‡ç»/ç­†è¨˜è³‡æ–™ (æ¯ä¸€ç¯‡è«‹è¨˜å¾— **æŒ‰ Enter æ›è¡Œ**)")
raw_text = st.text_area("è³‡æ–™è¼¸å…¥å€", height=250, placeholder="ç¬¬ä¸€ç¯‡æ–‡ç»å…§å®¹...\nç¬¬äºŒç¯‡æ–‡ç»å…§å®¹...\n(ä¸éœ€è¦å¹´ä»½ï¼Œåªè¦æ›è¡Œå°±å¥½)")

# --- 2. æ ¸å¿ƒï¼šå¯¬é¬†åˆ‡å‰²é‚è¼¯ (è§£æ±ºæ‰¾ä¸åˆ°å¹´ä»½çš„å•é¡Œ) ---
def loose_parse(text):
    lines = text.strip().split('\n')
    literature_list = []
    
    for line in lines:
        line = line.strip()
        if len(line) < 4: continue # éæ¿¾å¤ªçŸ­çš„
        
        # è‡ªå‹•æŠ“å–å‰15å­—ç•¶ä½œä»£è™Ÿ
        name_guess = line[:15] + "..." if len(line) > 15 else line
        
        literature_list.append({
            "author": name_guess, 
            "abstract": line
        })
    return literature_list

# --- 3. æ ¸å¿ƒï¼šæ”¹è‰¯ç‰ˆé—œéµå­—ç®—æ³• (è§£æ±ºã€ŒæœŸä¸‹åˆã€é€™ç¨®æ€ªå­—) ---
def smart_keyword_extraction(text_list, top_n=15):
    # æ¥åˆæ‰€æœ‰æ–‡å­—
    full_text = " ".join([item['abstract'] for item in text_list])
    
    # åªç•™ä¸­æ–‡
    chinese_only = re.sub(r'[^\u4e00-\u9fa5]', '', full_text)
    
    words = []
    
    # --- ğŸ”¥ å¼·åŠ›åƒåœ¾è©é»‘åå–® (è§£æ±ºä½ çš„æˆªåœ–å•é¡Œ) ---
    stop_chars = set(['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'èˆ‡', 'è‘—', 'æˆ–', 'ä¸€å€‹', 'æ²’æœ‰', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'å°æ–¼', 'é—œæ–¼', 'ä½†æ˜¯', 'å› ç‚º', 'æ‰€ä»¥', 'å¦‚æœ', 'é›–ç„¶', 'ä»¥åŠ', 'é€é', 'é€²è¡Œ', 'ä½¿ç”¨', 'åˆ†æ', 'ç ”ç©¶', 'æ¢è¨', 'æå‡º', 'çµæœ', 'é¡¯ç¤º', 'ç™¼ç¾', 'æœ¬æ–‡', 'æ‘˜è¦', 'æ–‡ç»', 'è³‡æ–™', 'æ•¸æ“š', 'å ±å‘Š'])
    
    # é‡å°æ™‚é–“æ—¥æœŸçš„éæ¿¾ (è§£æ±º "æ—¥æœŸ", "æœŸä¸‹", "ä¸‹åˆ" ç­‰å•é¡Œ)
    time_words = set(['æ—¥æœŸ', 'æ™‚é–“', 'ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š', 'ä»Šå¤©', 'æ˜å¤©', 'å¾Œå¤©', 'æ˜¨å¤©', 'æ˜ŸæœŸ', 'ç¦®æ‹œ', 'æœˆä»½', 'å¹´ä»½', 'å¹´åº¦', 'æœŸé–“', 'é–‹å§‹', 'çµæŸ', 'ç¾åœ¨', 'ç›®å‰', 'æœªä¾†', 'éå»', 'æœŸä¸‹', 'æœŸä¸­', 'æœŸä¸Š'])

    # ç°¡å–®åˆ‡è© (N-gram)
    for i in range(len(chinese_only)):
        # æŠ“ 2 å­—è©
        if i + 2 <= len(chinese_only):
            w = chinese_only[i:i+2]
            if w not in stop_chars and w not in time_words: 
                words.append(w)
        
        # æŠ“ 3 å­—è© (æº–å‰‡é€šå¸¸æ˜¯3-4å­—ï¼Œçµ¦å®ƒå¤šä¸€é»æ©Ÿæœƒ)
        if i + 3 <= len(chinese_only):
            w = chinese_only[i:i+3]
            if w not in stop_chars and w not in time_words: 
                words.append(w) # è®“å®ƒé‡è¤‡åŠ å…¥ï¼Œå¢åŠ æ¬Šé‡
                words.append(w) 

        # æŠ“ 4 å­—è©
        if i + 4 <= len(chinese_only):
            w = chinese_only[i:i+4]
            if w not in stop_chars and w not in time_words: 
                words.append(w)
                words.append(w) # æ¬Šé‡åŠ å€

    # çµ±è¨ˆé »ç‡
    counter = Counter(words)
    
    # éæ¿¾æ‰é »ç‡å¤ªä½(åªå‡ºç¾ä¸€æ¬¡)çš„é›œè¨Š
    filtered_keywords = [w for w, c in counter.most_common(50) if c > 1]
    
    # å›å‚³å‰ N å€‹
    return filtered_keywords[:top_n]

# --- 4. åŸ·è¡Œèˆ‡é¡¯ç¤º ---
if st.button("ğŸš€ è‡ªå‹•å»ºæ§‹æº–å‰‡ä¸¦åˆ†æ", type="primary"):
    if not raw_text:
        st.warning("è«‹å…ˆè²¼ä¸Šè³‡æ–™ï¼")
    else:
        # A. åˆ‡å‰²è³‡æ–™
        lit_data = loose_parse(raw_text)
        
        if not lit_data:
            st.error("ç„¡æ³•è­˜åˆ¥è³‡æ–™ï¼Œè«‹ç¢ºèªæœ‰æŒ‰ Enter æ›è¡Œã€‚")
        else:
            st.success(f"âœ… æˆåŠŸè®€å– {len(lit_data)} ç­†è³‡æ–™ï¼Œæ­£åœ¨å»ºæ§‹æº–å‰‡...")
            
            # B. é‹ç®—é—œéµå­—
            auto_keywords = smart_keyword_extraction(lit_data)
            
            # è‹¥é‹ç®—çµæœå¾ˆå°‘ï¼Œçµ¦äºˆé è¨­æç¤º
            if not auto_keywords:
                auto_keywords = ["(è³‡æ–™é‡å¤ªå°‘ï¼Œç„¡æ³•çµ±è¨ˆå‡ºé¡¯è‘—æº–å‰‡ï¼Œè«‹æ‰‹å‹•è¼¸å…¥)"]

            # C. è®“ä½¿ç”¨è€…ç¢ºèªèˆ‡ä¿®æ”¹ (é€™æ˜¯é—œéµæ­¥é©Ÿ)
            st.divider()
            st.subheader("1ï¸âƒ£ ç³»çµ±å»ºè­°çš„æº–å‰‡ (å¯ä¿®æ”¹)")
            
            col_sel, col_add = st.columns([2, 1])
            with col_sel:
                selected_keywords = st.multiselect(
                    "è«‹å‹¾é¸æ‚¨è¦ä¿ç•™çš„æº–å‰‡ï¼š",
                    options=auto_keywords,
                    default=auto_keywords
                )
            with col_add:
                manual_add = st.text_input("æ‰‹å‹•è£œå……æº–å‰‡ (ç”¨ç©ºç™½éš”é–‹)ï¼š", placeholder="ä¾‹å¦‚ï¼šESG ç²åˆ©èƒ½åŠ›")
            
            # åˆä½µ
            final_keywords = selected_keywords
            if manual_add:
                final_keywords = manual_add.split() + final_keywords
            
            # å»é™¤é‡è¤‡
            final_keywords = list(dict.fromkeys(final_keywords))

            if final_keywords:
                # D. ç”ŸæˆçŸ©é™£ (è·Ÿä¹‹å‰ä¸€æ¨£çš„é‚è¼¯)
                matrix = {}
                labels = []
                full_names = []
                
                def get_label(index):
                    if index < 26: return string.ascii_uppercase[index]
                    else: return f"{string.ascii_uppercase[index // 26 - 1]}{string.ascii_uppercase[index % 26]}"

                for i, item in enumerate(lit_data):
                    label = get_label(i)
                    labels.append(label)
                    full_names.append(item['author'])
                    
                    col_res = []
                    for kw in final_keywords:
                        if kw in item['abstract']:
                            col_res.append("â—‹")
                        else:
                            col_res.append("")
                    matrix[label] = col_res
                
                # E. é¡¯ç¤ºçµæœ
                df_matrix = pd.DataFrame(matrix, index=final_keywords)
                df_legend = pd.DataFrame({"ä»£è™Ÿ": labels, "å°æ‡‰å…§å®¹": full_names})
                
                st.divider()
                st.subheader("2ï¸âƒ£ åˆ†æçµæœ")
                
                c1, c2 = st.columns([2, 1])
                with c1:
                    st.dataframe(df_matrix, use_container_width=True)
                with c2:
                    st.dataframe(df_legend, hide_index=True, use_container_width=True)
                
                # F. æ™ºæ…§ä¸‹è¼‰ (è§£æ±ºç´…è‰²éŒ¯èª¤)
                output = BytesIO()
                download_ready = False
                file_name = "matrix.csv"
                mime_type = "text/csv"
                
                try:
                    import xlsxwriter
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        df_matrix.to_excel(writer, sheet_name='çŸ©é™£')
                        df_legend.to_excel(writer, sheet_name='å°ç…§è¡¨')
                    file_name = "analysis.xlsx"
                    mime_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    download_ready = True
                except ImportError:
                    # å¦‚æœæ²’è£ xlsxwriterï¼Œå°±ä¸‹è¼‰ CSVï¼Œä¸å ±éŒ¯ï¼
                    st.warning("âš ï¸ ç³»çµ±åµæ¸¬åˆ°æœªå®‰è£ xlsxwriterï¼Œå°‡è‡ªå‹•æ”¹ç‚ºä¸‹è¼‰ CSV æ ¼å¼ã€‚")
                    output = BytesIO()
                    df_matrix.to_csv(output, encoding='utf-8-sig')
                    download_ready = True

                if download_ready:
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è¼‰å ±è¡¨ ({file_name.split('.')[-1]})", 
                        data=output.getvalue(), 
                        file_name=file_name,
                        mime=mime_type,
                        type="primary"
                    )
