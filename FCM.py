import streamlit as st
import pandas as pd
import requests
import json
import string
from io import BytesIO

# --- 1. åŸºç¤è¨­å®š ---
st.set_page_config(page_title="MCDM å…©éšæ®µæ”¶æ–‚åˆ†æå™¨", layout="wide", page_icon="ğŸ§¬")

# --- 2. å´é‚Šæ¬„ï¼šåƒæ•¸è¨­å®š ---
with st.sidebar:
    st.header("ğŸ§¬ å…©éšæ®µæ”¶æ–‚è¨­å®š")
    st.info("æ­¤ç³»çµ±å°‡åŸ·è¡Œï¼šç™¼æ•£ (æ‰¾å‡ºå¤§é‡ç´°é …) -> æ”¶æ–‚ (æ­¸ç´æ ¸å¿ƒæº–å‰‡) çš„é‚è¼¯é‹ç®—ã€‚")
    
    api_key = st.text_input("Google API Key", type="password")
    
    st.divider()
    
    # ç ”ç©¶é¡Œç›®
    thesis_topic = st.text_input("æ‚¨çš„è«–æ–‡é¡Œç›®ï¼š", value="é¤é£²æ¥­å°å…¥ AI æœå‹™ä¹‹è©•ä¼°æº–å‰‡")
    
    # å…©éšæ®µåƒæ•¸
    c1, c2 = st.columns(2)
    with c1:
        pool_size = st.number_input("ç¬¬ä¸€æ­¥ï¼šå»£æ³›åˆ—å‡º", value=50, help="å¸Œæœ› AI å…ˆå¾æ–‡ç»æŠ“å‡ºå¤šå°‘å€‹ç´°é …")
    with c2:
        target_size = st.number_input("ç¬¬äºŒæ­¥ï¼šæ”¶æ–‚æˆ", value=15, help="æœ€å¾Œå¸Œæœ›æ­¸ç´æˆå¹¾å€‹ä¸»è¦æº–å‰‡")

# --- 3. è‡ªå‹•å°‹æ‰¾å¯ç”¨æ¨¡å‹ ---
def get_best_model(key):
    # å„ªå…ˆå˜—è©¦ Pro æ¨¡å‹ï¼Œå› ç‚ºæ”¶æ–‚é‚è¼¯éœ€è¦è¼ƒå¼·çš„æ¨ç†èƒ½åŠ›
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={key}"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            models = response.json().get('models', [])
            # å„ªå…ˆé †åº: 1.5-Pro -> 1.5-Flash
            for m in models:
                if 'gemini-1.5-pro' in m['name']: return m['name']
            for m in models:
                if 'gemini-1.5-flash' in m['name']: return m['name']
            for m in models:
                if 'gemini' in m['name']: return m['name']
        return "models/gemini-1.5-pro"
    except:
        return "models/gemini-1.5-flash"

# --- 4. æ ¸å¿ƒåˆ†æé‚è¼¯ (é›™éšæ®µæ”¶æ–‚ Prompt) ---
def run_convergence_analysis(text, key, model_name, topic, pool_n, target_n):
    url = f"https://generativelanguage.googleapis.com/v1beta/{model_name}:generateContent?key={key}"
    headers = {'Content-Type': 'application/json'}
    
    # é€™å€‹ Prompt æ˜¯æ•´å€‹ç¨‹å¼çš„éˆé­‚
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹ MCDM ç ”ç©¶æ–¹æ³•çš„å°ˆå®¶ã€‚
    ã€ç ”ç©¶é¡Œç›®ã€‘ï¼š{topic}
    ã€ä»»å‹™ç›®æ¨™ã€‘ï¼šè«‹åŸ·è¡Œã€Œå…©éšæ®µæº–å‰‡ç¯©é¸æ³•ã€ã€‚
    
    ã€éšæ®µä¸€ï¼šç™¼æ•£ (Brainstorming)ã€‘
    è«‹å…ˆé–±è®€æ–‡ç»ï¼Œå¾ä¸­ç›¡å¯èƒ½æ‰¾å‡ºç´„ {pool_n} å€‹èˆ‡é¡Œç›®ç›¸é—œçš„ã€ŒåŸå§‹ç´°é …æº–å‰‡ (Raw Criteria)ã€ã€‚

    ã€éšæ®µäºŒï¼šæ”¶æ–‚ (Convergence)ã€‘
    è«‹é‹ç”¨ä½ çš„é‚è¼¯ï¼Œå°‡ä¸Šè¿°ç´°é …æº–å‰‡é€²è¡Œåˆä½µã€åˆ†é¡ï¼Œæ­¸ç´å‡ºæœ€å…·ä»£è¡¨æ€§çš„ {target_n} å€‹ã€Œæœ€çµ‚æº–å‰‡ (Final Criteria)ã€ã€‚
    
    ã€è¼¸å‡ºè¦æ±‚ã€‘ï¼š
    1. å»ºç«‹çŸ©é™£ï¼šæ¨™ç¤ºæ¯ä¸€ç¯‡æ–‡ç»æ˜¯å¦æåˆ°äº†è©²ã€Œæœ€çµ‚æº–å‰‡ã€ã€‚
    2. è§£é‡‹é‚è¼¯ï¼šå¿…é ˆè©³ç´°èªªæ˜æ¯å€‹ã€Œæœ€çµ‚æº–å‰‡ã€æ˜¯ç”±å“ªäº›ã€ŒåŸå§‹ç´°é …ã€åˆä½µè€Œä¾†ï¼Œä»¥åŠåˆä½µçš„ç†ç”±ã€‚
    
    è«‹ç›´æ¥å›å‚³ç´” JSON æ ¼å¼ (ä¸è¦ Markdown)ï¼Œçµæ§‹åš´æ ¼å¦‚ä¸‹ï¼š
    {{
      "final_dimensions": [
        {{
          "id": 1,
          "name": "æœ€çµ‚æº–å‰‡åç¨± (ä¾‹å¦‚ï¼šç‡Ÿé‹æˆæœ¬)",
          "composition_logic": "æœ¬æº–å‰‡åˆä½µäº†ï¼šåŸå§‹ç´°é …Aã€åŸå§‹ç´°é …Bã€‚åŸå› ï¼šå®ƒå€‘éƒ½å±¬æ–¼æˆæœ¬çµæ§‹...",
          "matched_papers_indices": [0, 2] // ä»£è¡¨ç¬¬1ç¯‡å’Œç¬¬3ç¯‡æ–‡ç»æœ‰æåˆ°æ­¤æº–å‰‡
        }},
        ... (å…± {target_n} å€‹)
      ],
      "papers": [
        "æ–‡ç»1çš„APAæ ¼å¼ citation...",
        "æ–‡ç»2çš„APAæ ¼å¼ citation...",
        ...
      ]
    }}

    ã€åŸå§‹æ–‡ç»è³‡æ–™ã€‘ï¼š
    {text[:14000]}
    """
    
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            res_text = response.json()['candidates'][0]['content']['parts'][0]['text']
            clean_json = res_text.replace("```json", "").replace("```", "").strip()
            return "OK", json.loads(clean_json)
        else:
            return "ERROR", f"éŒ¯èª¤ ({response.status_code}): {response.text}"
    except Exception as e:
        return "ERROR", str(e)

# --- 5. ä¸»ç•«é¢ ---
st.title("ğŸ§¬ MCDM æº–å‰‡ï¼šç™¼æ•£èˆ‡æ”¶æ–‚å·¥ä½œå€")

raw_text = st.text_area("è«‹åœ¨æ­¤è²¼ä¸Šæ–‡ç»æ‘˜è¦ (AI æœƒå…ˆæŠ“å¤§æ± å­ï¼Œå†æ”¶æ–‚æˆç²¾è¯)ï¼š", height=300)

if st.button("ğŸš€ åŸ·è¡Œæ”¶æ–‚é‹ç®—", type="primary"):
    if not api_key:
        st.error("âŒ è«‹å…ˆè²¼ä¸Š API Keyï¼")
    elif not raw_text:
        st.warning("âš ï¸ è«‹è¼¸å…¥æ–‡ç»è³‡æ–™ï¼")
    else:
        status_box = st.empty()
        status_box.info(f"ğŸ” AI æ­£åœ¨æ€è€ƒï¼šå…ˆæ‰¾å‡º {pool_size} å€‹ç´°é …ï¼Œå†é‚è¼¯æ­¸ç´ç‚º {target_size} å€‹ä¸»æº–å‰‡...")
        
        valid_model = get_best_model(api_key)
        
        # åŸ·è¡Œåˆ†æ
        status, result_data = run_convergence_analysis(raw_text, api_key, valid_model, thesis_topic, pool_size, target_size)
        
        if status == "OK":
            status_box.success("âœ… æ”¶æ–‚å®Œæˆï¼é‚è¼¯çŸ©é™£å·²ç”Ÿæˆã€‚")
            
            try:
                # è§£æè³‡æ–™
                dimensions = result_data.get("final_dimensions", [])
                papers_list = result_data.get("papers", [])
                
                # æº–å‚™ DataFrame çš„è³‡æ–™å®¹å™¨
                # æ¬„ä½é †åºï¼šåºè™Ÿ | æœ€çµ‚æº–å‰‡ | [æ–‡ç»A] | [æ–‡ç»B]... | æ”¶æ–‚é‚è¼¯èªªæ˜ (æœ€å³é‚Š)
                
                rows = []
                paper_labels = [string.ascii_uppercase[i % 26] for i in range(len(papers_list))]
                
                for dim in dimensions:
                    row_data = {}
                    # 1. åºè™Ÿ
                    row_data["åºè™Ÿ"] = dim.get("id")
                    # 2. æº–å‰‡åç¨±
                    row_data["æœ€çµ‚è©•ä¼°æº–å‰‡"] = dim.get("name")
                    
                    # 3. æ–‡ç»çŸ©é™£ (ä¸­é–“)
                    matched_indices = dim.get("matched_papers_indices", [])
                    for idx, label in enumerate(paper_labels):
                        row_data[label] = "â—" if idx in matched_indices else ""
                    
                    # 4. æ”¶æ–‚é‚è¼¯ (æœ€å³é‚Š - é€™æ˜¯ä½ ç‰¹åˆ¥è¦æ±‚çš„)
                    row_data["æ”¶æ–‚é‚è¼¯èˆ‡åŸå§‹ç´°é …ä¾†æº"] = dim.get("composition_logic")
                    
                    rows.append(row_data)
                
                # å»ºç«‹ä¸»è¡¨
                df_main = pd.DataFrame(rows)
                
                # å»ºç«‹æ–‡ç»å°ç…§è¡¨
                df_papers = pd.DataFrame({
                    "ä»£è™Ÿ": paper_labels,
                    "æ–‡ç»ä¾†æº (APA)": papers_list
                })
                
                st.divider()
                
                # é¡¯ç¤ºå€åŸŸ
                st.subheader("ğŸ“Š æ”¶æ–‚çµæœçŸ©é™£")
                st.markdown("è«‹å‘å³æ»‘å‹•è¡¨æ ¼æŸ¥çœ‹æœ€å³å´çš„**ã€Œæ”¶æ–‚é‚è¼¯ã€**æ¬„ä½ ğŸ‘‰")
                st.dataframe(df_main, hide_index=True, use_container_width=True)
                
                st.subheader("ğŸ“ æ–‡ç»ä¾†æºå°ç…§")
                st.dataframe(df_papers, hide_index=True, use_container_width=True)
                
                # ä¸‹è¼‰åŠŸèƒ½
                output = BytesIO()
                try:
                    import xlsxwriter
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        df_main.to_excel(writer, sheet_name='æ”¶æ–‚çŸ©é™£', index=False)
                        df_papers.to_excel(writer, sheet_name='æ–‡ç»ä¾†æº', index=False)
                    st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel (å«é‚è¼¯èªªæ˜)", output.getvalue(), "mcdm_convergence.xlsx", type="primary")
                except:
                    st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV", df_main.to_csv().encode('utf-8-sig'), "mcdm_convergence.csv")

            except Exception as e:
                st.error(f"è³‡æ–™è§£æç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                st.json(result_data)
        else:
            status_box.error("åˆ†æå¤±æ•—")
            st.code(result_data)
